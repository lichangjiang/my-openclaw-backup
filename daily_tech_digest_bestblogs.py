#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æŠ€æœ¯æ‘˜è¦ - ä¿®å¤ BestBlogs è®¿é—®
ä½¿ç”¨ urllib ç›´æ¥è®¿é—®ï¼Œæ·»åŠ è¶…æ—¶å’Œé‡è¯•
"""

import json
import datetime
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from typing import List, Dict
import time

# RSS æºé…ç½®
RSS_SOURCES = {
    "bestblogs_featured": {
        "url": "https://www.bestblogs.dev/zh/feeds/rss?featured=y",
        "name": "BestBlogs ç²¾é€‰",
        "category": "programming",
    },
    "bestblogs_programming": {
        "url": "https://www.bestblogs.dev/zh/feeds/rss?category=programming&type=article",
        "name": "BestBlogs ç¼–ç¨‹æŠ€æœ¯",
        "category": "programming",
    },
    "bestblogs_ai": {
        "url": "https://www.bestblogs.dev/en/feeds/rss?category=ai&minScore=90",
        "name": "BestBlogs AI é«˜åˆ†",
        "category": "ai",
    },
    "bestblogs_product": {
        "url": "https://www.bestblogs.dev/zh/feeds/rss?category=product",
        "name": "BestBlogs äº§å“è®¾è®¡",
        "category": "product",
    },
}

ARTICLES_PER_DAY = 10
DEFAULT_RATIOS = {"programming": 3, "ai": 5, "product": 2}

def fetch_rss(url: str, timeout: int = 15, retries: int = 3) -> str:
    """è·å– RSS å†…å®¹ï¼Œå¸¦è¶…æ—¶å’Œé‡è¯•"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; OpenClaw/1.0)',
        'Accept': 'application/rss+xml, application/xml, text/xml',
    }
    
    for attempt in range(retries):
        try:
            request = urllib.request.Request(url, headers=headers)
            
            with urllib.request.urlopen(request, timeout=timeout) as response:
                content = response.read().decode('utf-8')
                print(f"     âœ… æˆåŠŸè·å–ï¼ˆ{len(content)} bytesï¼‰")
                return content
                
        except urllib.error.URLError as e:
            print(f"     âš ï¸ å°è¯• {attempt+1}/{retries} å¤±è´¥: {e}")
            if attempt < retries - 1:
                time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
        except Exception as e:
            print(f"     âš ï¸ å°è¯• {attempt+1}/{retries} å¤±è´¥: {e}")
            if attempt < retries - 1:
                time.sleep(1)
    
    return None

def parse_rss(xml_content: str, source_key: str, category: str) -> List[Dict]:
    """è§£æ RSS XML"""
    if not xml_content:
        return []
    
    articles = []
    
    try:
        root = ET.fromstring(xml_content)
        
        # å¤„ç†å‘½åç©ºé—´
        if root.tag.startswith('{'):
            ns = root.tag.split('{')[1].split('}')[0]
            ET.register_namespace('', ns)
            root = ET.fromstring(xml_content)
        
        # RSS 2.0 æ ¼å¼
        if root.tag.endswith('rss') or root.tag == 'rss':
            channel = root.find('channel')
            if channel is None:
                channel = root
            
            items = channel.findall('item')
            
            for item in items[:15]:  # å–å‰15ç¯‡
                title_elem = item.find('title')
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')
                desc_elem = item.find('description')
                
                if title_elem is not None and title_elem.text and link_elem is not None:
                    article = {
                        'title': title_elem.text.strip(),
                        'link': link_elem.text.strip() if link_elem.text else '',
                        'published': pub_date_elem.text if pub_date_elem is not None else '',
                        'summary': (desc_elem.text[:150] if desc_elem.text else '') if desc_elem is not None else '',
                        'source': source_key,
                        'category': category,
                    }
                    
                    # è®¡ç®—æ–°é²œåº¦
                    article['freshness_score'] = calculate_freshness(article.get('published', ''))
                    
                    # åªä¿ç•™æœ€è¿‘7å¤©çš„æ–‡ç« 
                    if article['freshness_score'] >= 6:
                        articles.append(article)
                    
    except Exception as e:
        print(f"     âš ï¸ è§£æå¤±è´¥: {e}")
    
    return articles

def calculate_freshness(published_date: str) -> int:
    """è®¡ç®—æ–°é²œåº¦ï¼ˆ0-10ï¼‰"""
    if not published_date:
        return 5
    
    try:
        from email.utils import parsedate_to_datetime
        parsed_date = parsedate_to_datetime(published_date)
        
        if parsed_date.tzinfo is None:
            parsed_date = parsed_date.replace(tzinfo=datetime.timezone.utc)
        
        delta = (datetime.datetime.now(datetime.timezone.utc) - parsed_date).days
        
        if delta <= 1:
            return 10
        elif delta <= 3:
            return 8
        elif delta <= 7:
            return 6
        elif delta <= 14:
            return 4
        elif delta <= 30:
            return 2
        else:
            return 0
            
    except:
        return 5

def fetch_articles() -> List[Dict]:
    """è·å–æ‰€æœ‰æ–‡ç« """
    all_articles = []
    
    print("ğŸ“¡ æ­£åœ¨è·å– BestBlogs RSS æº...")
    
    for source_key, source_config in RSS_SOURCES.items():
        try:
            print(f"  â†’ {source_config['name']}")
            
            # è·å– RSS
            xml_content = fetch_rss(source_config['url'], timeout=15, retries=2)
            
            if xml_content:
                # è§£æ
                articles = parse_rss(xml_content, source_key, source_config['category'])
                
                print(f"     ğŸ“Š è§£æåˆ° {len(articles)} ç¯‡æ–‡ç« ï¼ˆæœ€è¿‘7å¤©å†…ï¼‰")
                all_articles.extend(articles)
            else:
                print(f"     âŒ è·å–å¤±è´¥")
                
        except Exception as e:
            print(f"  âš ï¸ {source_config['name']} å¤„ç†å¤±è´¥: {e}")
    
    print(f"âœ… å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
    return all_articles

def select_articles(articles: List[Dict]) -> List[Dict]:
    """é€‰æ‹©æ–‡ç« """
    # æŒ‰æ–°é²œåº¦æ’åº
    articles.sort(key=lambda x: x.get('freshness_score', 0), reverse=True)
    
    # å–å‰10ç¯‡
    return articles[:ARTICLES_PER_DAY]

def generate_digest(articles: List[Dict]) -> str:
    """ç”Ÿæˆæ‘˜è¦"""
    now = datetime.datetime.now(datetime.timezone.utc)
    beijing_time = now + datetime.timedelta(hours=8)
    date_str = beijing_time.strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    digest = f"ğŸ“… {date_str} æ¯æ—¥æŠ€æœ¯æ‘˜è¦\n\n"
    digest += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    digest += f"ğŸ”¥ ä»Šæ—¥ç²¾é€‰ï¼ˆ{len(articles)} ç¯‡ï¼‰\n\n"
    
    # æŒ‰åˆ†ç±»
    categories = [
        ("programming", "ğŸ’» ç¼–ç¨‹æŠ€æœ¯"),
        ("ai", "ğŸ¤– AI å‰æ²¿"),
        ("product", "ğŸ¨ äº§å“è®¾è®¡")
    ]
    
    for cat_key, cat_name in categories:
        cat_articles = [a for a in articles if a.get('category') == cat_key]
        if not cat_articles: continue
        
        digest += f"### {cat_name}ï¼ˆ{len(cat_articles)} ç¯‡ï¼‰\n\n"
        
        for i, article in enumerate(cat_articles, 1):
            title_display = article['title'][:60] + ('...' if len(article['title']) > 60 else '')
            source_name = RSS_SOURCES.get(article['source'], {}).get('name', 'Unknown')
            
            digest += f"{i}. **{title_display}**\n"
            digest += f"   * æ¥æºï¼š{source_name}\n"
            
            # æ˜¾ç¤ºå‘å¸ƒæ—¶é—´
            published = article.get('published', '')
            if published:
                digest += f"   * å‘å¸ƒï¼š{published[:20]}...\n"
            
            digest += f"   * é“¾æ¥ï¼š{article['link']}\n\n"
    
    digest += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
    digest += "æ¥æºï¼šBestBlogs.dev | ç®¡ç†è®¢é˜…ï¼šhttps://www.bestblogs.dev/#subscribe"
    
    return digest

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ¯æ—¥æŠ€æœ¯æ‘˜è¦ï¼ˆBestBlogs æºï¼‰")
    print("=" * 50)
    
    # è·å–æ–‡ç« 
    articles = fetch_articles()
    
    if not articles:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–‡ç« ")
        return
    
    # é€‰æ‹©æ–‡ç« 
    selected_articles = select_articles(articles)
    
    # ç”Ÿæˆæ‘˜è¦
    digest = generate_digest(selected_articles)
    
    print("\nğŸ“ æ‘˜è¦ç”Ÿæˆå®Œæˆ")
    print(digest)

if __name__ == "__main__":
    import os
    main()
