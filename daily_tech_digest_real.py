#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æŠ€æœ¯æ‘˜è¦ç”Ÿæˆè„šæœ¬ - çœŸå® RSS æºç‰ˆæœ¬
ä¿®å¤é“¾æ¥æ—¶æ•ˆæ€§é—®é¢˜
"""

import json
import datetime
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from typing import List, Dict
import time

# RSS æºé…ç½®
RSS_SOURCES = {
    "bestblogs_ai": {
        "url": "https://www.bestblogs.dev/en/feeds/rss?category=ai&minScore=90",
        "name": "BestBlogs AI é«˜åˆ†",
        "category": "ai",
    },
    "bestblogs_programming": {
        "url": "https://www.bestblogs.dev/zh/feeds/rss?category=programming&type=article",
        "name": "BestBlogs ç¼–ç¨‹æŠ€æœ¯",
        "category": "programming",
    },
    "bestblogs_product": {
        "url": "https://www.bestblogs.dev/zh/feeds/rss?category=product",
        "name": "BestBlogs äº§å“è®¾è®¡",
        "category": "product",
    },
}

USER_PREFERENCES_FILE = "/home/lichangjiang/.openclaw/workspace/user_preferences.json"
ARTICLES_PER_DAY = 10
DEFAULT_RATIOS = {"programming": 3, "ai": 5, "product": 2}

def load_user_preferences() -> Dict:
    try:
        if os.path.exists(USER_PREFERENCES_FILE):
            with open(USER_PREFERENCES_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ç”¨æˆ·åå¥½å¤±è´¥: {e}")
        return {}

def get_user_ratios() -> Dict[str, int]:
    preferences = load_user_preferences()
    clicks = preferences.get('clicks', [])
    if not clicks:
        return DEFAULT_RATIOS.copy()
    
    category_counts = {"programming": 0, "ai": 0, "product": 0}
    for click in clicks:
        category = click.get('category', 'unknown')
        if category in category_counts:
            category_counts[category] += 1
            
    total_clicks = sum(category_counts.values()) or 1
    if total_clicks == 0:
        return DEFAULT_RATIOS.copy()
        
    ratios = {}
    for category, count in category_counts.items():
        articles = (count / total_clicks) * ARTICLES_PER_DAY
        ratios[category] = max(2, round(articles))
    return ratios

def get_user_topics() -> List[str]:
    preferences = load_user_preferences()
    clicks = preferences.get('clicks', [])
    topic_counts = {}
    
    keywords = ['python', 'rust', 'go', 'java', 'react', 'vue', 'docker',
                'kubernetes', 'k8s', 'ai', 'ml', 'security', 'linux']
    
    for click in clicks:
        title = click.get('title', '')
        for keyword in keywords:
            if keyword.lower() in title.lower():
                topic_counts[keyword] = topic_counts.get(keyword, 0) + 1
                
    sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    return [topic for topic, count in sorted_topics]

def fetch_rss(url: str, timeout: int = 10) -> str:
    """è·å– RSS å†…å®¹ï¼Œå¸¦è¶…æ—¶å’Œé‡è¯•"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; OpenClaw/1.0)'
    }
    
    request = urllib.request.Request(url, headers=headers)
    
    try:
        with urllib.request.urlopen(request, timeout=timeout) as response:
            return response.read().decode('utf-8')
    except Exception as e:
        print(f"  âš ï¸ è·å–å¤±è´¥: {e}")
        return None

def parse_rss_xml(xml_content: str, source_key: str, category: str) -> List[Dict]:
    """è§£æ RSS XML å†…å®¹"""
    if not xml_content:
        return []
    
    articles = []
    
    try:
        root = ET.fromstring(xml_content)
        
        # RSS 2.0 æ ¼å¼
        if root.tag == 'rss':
            channel = root.find('channel')
            items = channel.findall('item') if channel is not None else []
            
            for item in items[:20]:  # åªå–å‰20ç¯‡æ–‡ç« 
                title = item.find('title')
                link = item.find('link')
                pub_date = item.find('pubDate')
                description = item.find('description')
                
                if title is not None and link is not None:
                    article = {
                        'title': title.text or '',
                        'link': link.text or '',
                        'published': pub_date.text if pub_date is not None else '',
                        'summary': (description.text[:200] if description else '') if description else '',
                        'source': source_key,
                        'category': category,
                    }
                    
                    # è®¡ç®—æ–‡ç« æ–°é²œåº¦ï¼ˆ0-10åˆ†ï¼‰
                    article['freshness_score'] = calculate_freshness(article.get('published', ''))
                    articles.append(article)
        
        # Atom æ ¼å¼
        elif root.tag.endswith('feed'):
            items = root.findall('.//{http://www.w3.org/2005/Atom}entry')
            
            for item in items[:20]:
                title = item.find('{http://www.w3.org/2005/Atom}title')
                link = item.find('{http://www.w3.org/2005/Atom}link')
                pub_date = item.find('{http://www.w3.org/2005/Atom}published')
                content = item.find('{http://www.w3.org/2005/Atom}content')
                
                if title is not None and link is not None:
                    article = {
                        'title': title.text or '',
                        'link': link.get('href', ''),
                        'published': pub_date.text if pub_date is not None else '',
                        'summary': (content.text[:200] if content else '') if content else '',
                        'source': source_key,
                        'category': category,
                    }
                    
                    article['freshness_score'] = calculate_freshness(article.get('published', ''))
                    articles.append(article)
                    
    except Exception as e:
        print(f"  âš ï¸ è§£æå¤±è´¥: {e}")
    
    return articles

def calculate_freshness(published_date: str) -> int:
    """è®¡ç®—æ–‡ç« æ–°é²œåº¦åˆ†æ•°ï¼ˆ0-10ï¼‰"""
    if not published_date:
        return 5  # é»˜è®¤ä¸­ç­‰åˆ†
    
    try:
        # å°è¯•è§£æå¤šç§æ—¥æœŸæ ¼å¼
        date_formats = [
            '%a, %d %b %Y %H:%M:%S %Z',
            '%a, %d %b %Y %H:%M:%S %z',
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y-%m-%dT%H:%M:%S%z',
            '%Y-%m-%dT%H:%M:%S.%fZ',
        ]
        
        parsed_date = None
        for fmt in date_formats:
            try:
                parsed_date = datetime.datetime.strptime(published_date, fmt)
                break
            except ValueError:
                continue
        
        if parsed_date is None:
            return 5
        
        # è®¡ç®—å¤©æ•°å·®
        now = datetime.datetime.now(datetime.timezone.utc)
        # å¦‚æœ parsed_date æ²¡æœ‰ timezoneï¼Œå‡è®¾æ˜¯ UTC
        if parsed_date.tzinfo is None:
            parsed_date = parsed_date.replace(tzinfo=datetime.timezone.utc)
        
        delta = (now - parsed_date).days
        
        # æ ¹æ®å¤©æ•°ç»™åˆ†
        if delta <= 1:
            return 10
        elif delta <= 3:
            return 8
        elif delta <= 7:
            return 6
        elif delta <= 14:
            return 4
        elif delta <= 30:
            return 2
        else:
            return 0
            
    except Exception:
        return 5

def fetch_articles() -> List[Dict]:
    """ä»æ‰€æœ‰ RSS æºè·å–æ–‡ç« """
    all_articles = []
    
    print("ğŸ“¡ æ­£åœ¨è·å– RSS æº...")
    
    for source_key, source_config in RSS_SOURCES.items():
        try:
            print(f"  â†’ {source_config['name']}")
            
            # è·å– RSS å†…å®¹
            xml_content = fetch_rss(source_config['url'], timeout=10)
            
            # è§£æ XML
            articles = parse_rss_xml(xml_content, source_key, source_config['category'])
            
            all_articles.extend(articles)
            print(f"     âœ… è·å– {len(articles)} ç¯‡æ–‡ç« ")
            
        except Exception as e:
            print(f"  âš ï¸ {source_config['name']} è·å–å¤±è´¥: {e}")
    
    print(f"âœ… å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
    return all_articles

def select_articles(articles: List[Dict], user_ratios: Dict[str, int], user_topics: List[str]) -> Dict[str, List[Dict]]:
    """é€‰æ‹©æ–‡ç« """
    # è®¡ç®—ç»¼åˆè¯„åˆ†
    for article in articles:
        # åŸºç¡€åˆ† + æ–°é²œåº¦ + ä¸»é¢˜ç›¸å…³åº¦
        score = 50  # åŸºç¡€åˆ†
        score += article.get('freshness_score', 0)
        
        # ä¸»é¢˜ç›¸å…³åº¦ï¼ˆå¦‚æœåŒ¹é…çƒ­é—¨ä¸»é¢˜ï¼‰
        for topic in user_topics:
            if topic.lower() in article.get('title', '').lower():
                score += 10
                break
        
        article['score'] = score
    
    # æŒ‰åˆ†æ•°æ’åº
    articles.sort(key=lambda x: x['score'], reverse=True)
    
    # æŒ‰ç±»åˆ«åˆ†é…
    selected_articles = {"programming": [], "ai": [], "product": []}
    
    for article in articles:
        category = article.get('category', 'programming')
        if category in selected_articles:
            if len(selected_articles[category]) < user_ratios.get(category, 3):
                selected_articles[category].append(article)
    
    # ç¡®ä¿æ€»æ•°
    total = sum(len(v) for v in selected_articles.values())
    if total < ARTICLES_PER_DAY:
        for article in articles:
            if not any(article in v for v in selected_articles.values()):
                category = article.get('category', 'programming')
                if category in selected_articles:
                    selected_articles[category].append(article)
                    if sum(len(v) for v in selected_articles.values()) >= ARTICLES_PER_DAY:
                        break
    
    return selected_articles

def generate_digest(selected_articles: Dict[str, List[Dict]], user_topics: List[str]) -> str:
    """ç”Ÿæˆæ‘˜è¦"""
    now = datetime.datetime.now(datetime.timezone.utc)
    beijing_time = now + datetime.timedelta(hours=8)
    date_str = beijing_time.strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    digest = f"ğŸ“… {date_str} æ¯æ—¥æŠ€æœ¯æ‘˜è¦\n\n"
    digest += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    digest += f"ğŸ”¥ ä»Šæ—¥ç²¾é€‰ï¼ˆ{sum(len(v) for v in selected_articles.values())} ç¯‡ï¼‰\n\n"
    
    categories = [
        ("programming", "ğŸ’» ç¼–ç¨‹æŠ€æœ¯"),
        ("ai", "ğŸ¤– AI å‰æ²¿"),
        ("product", "ğŸ¨ äº§å“è®¾è®¡")
    ]
    
    for cat_key, cat_name in categories:
        articles = selected_articles.get(cat_key, [])
        if not articles: continue
        
        digest += f"### {cat_name}ï¼ˆ{len(articles)} ç¯‡ï¼‰\n\n"
        
        for i, article in enumerate(articles, 1):
            # æ˜¾ç¤ºå‰60ä¸ªå­—ç¬¦ï¼Œå¦‚æœè¶…è¿‡åˆ™åŠ çœç•¥å·
            title_display = article['title'][:60] + ('...' if len(article['title']) > 60 else '')
            
            digest += f"{i}. **{title_display}**\n"
            digest += f"   * æ¥æºï¼š{RSS_SOURCES.get(article['source'], {}).get('name', 'Unknown')}\n"
            
            # æ˜¾ç¤ºå‘å¸ƒæ—¶é—´
            published = article.get('published', '')
            if published:
                digest += f"   * å‘å¸ƒï¼š{published[:20]}...\n"
            
            digest += f"   * é“¾æ¥ï¼š{article['link']}\n\n"
    
    digest += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nğŸ’¡ ä¸ªæ€§åŒ–æç¤º\n"
    digest += f"**çƒ­é—¨ä¸»é¢˜ï¼š** {', '.join(user_topics) if user_topics else 'æš‚æ— æ•°æ®'}\n"
    digest += "\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\næ¥æºï¼šBestBlogs.dev | ç®¡ç†è®¢é˜…ï¼šhttps://www.bestblogs.dev/#subscribe"
    
    return digest

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ¯æ—¥æŠ€æœ¯æ‘˜è¦æ¨é€ç³»ç»Ÿï¼ˆçœŸå® RSS æºï¼‰")
    print("=" * 50)
    
    # è·å–ç”¨æˆ·åå¥½
    user_ratios = get_user_ratios()
    user_topics = get_user_topics()
    
    print(f"\nğŸ“Š æ¨èæ¯”ä¾‹: {user_ratios}")
    print(f"ğŸ“š çƒ­é—¨ä¸»é¢˜: {user_topics}")
    
    # è·å–æ–‡ç« 
    articles = fetch_articles()
    
    if not articles:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–‡ç« ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨é¢„è®¾æ–‡ç« 
        import daily_tech_digest_simple as backup
        digest = backup.generate_test_digest()
    else:
        # é€‰æ‹©æ–‡ç« 
        selected_articles = select_articles(articles, user_ratios, user_topics)
        
        # ç”Ÿæˆæ‘˜è¦
        digest = generate_digest(selected_articles, user_topics)
    
    print("\nğŸ“ æ‘˜è¦ç”Ÿæˆå®Œæˆ")
    print(digest)

if __name__ == "__main__":
    import os
    main()
