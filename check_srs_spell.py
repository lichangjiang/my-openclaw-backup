#!/usr/bin/env python3
"""
æ£€æŸ¥ SRS Word æ•°æ®åº“ä¸­æ‰€æœ‰å•è¯å’Œå¥å­çš„æ‹¼å†™å’Œç¿»è¯‘
é€šè¿‡ API è·å–æ•°æ®
"""

import json
import urllib.request
from datetime import datetime
import re

API_BASE_URL = "http://10.0.0.23:30080/api/knowledge_items"

def fetch_all_items():
    """åˆ†é¡µè·å–æ‰€æœ‰æ•°æ®"""
    all_items = []
    page = 1
    limit = 100
    total = None

    print("ğŸ“¡ æ­£åœ¨ä» API è·å–æ•°æ®...")

    while True:
        url = f"{API_BASE_URL}?page={page}&limit={limit}"
        print(f"  è·å–ç¬¬ {page} é¡µ...", end=" ")

        try:
            with urllib.request.urlopen(url) as response:
                data = json.loads(response.read().decode('utf-8'))

                if total is None:
                    total = data.get('total', 0)
                    print(f"(æ€»è®¡: {total} æ¡)")

                items = data.get('items', [])
                all_items.extend(items)
                print(f"å·²è·å– {len(items)} æ¡, ç´¯è®¡ {len(all_items)} æ¡")

                if len(all_items) >= total:
                    break

                page += 1
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®å¤±è´¥: {e}")
            break

    return all_items

def check_spelling_basic(word):
    """åŸºæœ¬çš„æ‹¼å†™æ£€æŸ¥"""
    issues = []

    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    if not word or word.strip() == '':
        issues.append("å•è¯ä¸ºç©º")
        return issues

    # æ£€æŸ¥æ˜¯å¦åŒ…å«éå­—æ¯å­—ç¬¦ï¼ˆä¸åŒ…æ‹¬å¸¸è§çš„è¿å­—ç¬¦ã€æ’‡å·ç­‰ï¼‰
    if word and not re.match(r"^[a-zA-Z\s\-']+$", word):
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æ‹¼å†™é”™è¯¯ï¼ˆå¤šä¸ªè¿ç»­ç›¸åŒå­—æ¯ï¼‰
        if re.search(r"(.)\1\1+", word):
            issues.append("åŒ…å«è¿ç»­é‡å¤çš„å­—æ¯")

    return issues

def check_sentence(sentence):
    """æ£€æŸ¥å¥å­"""
    issues = []

    if not sentence or sentence.strip() == '':
        issues.append("å¥å­ä¸ºç©º")
        return issues

    # æ£€æŸ¥å¤§å°å†™
    if sentence and not sentence[0].isupper():
        issues.append("é¦–å­—æ¯æœªå¤§å†™")

    # æ£€æŸ¥æ ‡ç‚¹ç¬¦å·
    if sentence and not sentence.endswith(('.', '!', '?')):
        issues.append("å¥æœ«ç¼ºå°‘æ ‡ç‚¹ç¬¦å·")

    return issues

def check_translation(translation):
    """æ£€æŸ¥ç¿»è¯‘"""
    issues = []

    if not translation or translation.strip() == '':
        issues.append("ç¿»è¯‘ä¸ºç©º")
        return issues

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¹±ç å­—ç¬¦ï¼ˆéä¸­æ–‡ã€è‹±æ–‡ã€å¸¸ç”¨æ ‡ç‚¹ï¼‰
    if translation:
        # å¸¸ç”¨å­—ç¬¦é›†ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹
        allowed = re.compile(r'^[\u4e00-\u9fff\u3400-\u4dbf\w\s\-\'.,:;?!""''()ã€ï¼Œã€‚ï¼›ï¼šï¼Ÿï¼ï¼ˆï¼‰]+$')
        if not allowed.match(translation):
            issues.append("ç¿»è¯‘åŒ…å«å¼‚å¸¸å­—ç¬¦")

    return issues

def main():
    print("=" * 80)
    print("SRS Word æ‹¼å†™å’Œç¿»è¯‘æ£€æŸ¥æŠ¥å‘Š")
    print("=" * 80)
    print(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print()

    # è·å–æ‰€æœ‰æ•°æ®
    items = fetch_all_items()

    if not items:
        print("âŒ æœªèƒ½è·å–ä»»ä½•æ•°æ®ï¼")
        return

    print(f"âœ… æˆåŠŸè·å– {len(items)} æ¡æ•°æ®")
    print()

    # ç»Ÿè®¡
    word_count = 0
    sentence_count = 0
    word_issues = []
    sentence_issues = []

    # åˆ†ææ•°æ®
    for item in items:
        item_type = item.get('item_type')
        content = item.get('content', '')
        phonetic = item.get('phonetic_symbol')
        definition = item.get('definition')
        translation = item.get('translation')
        item_id = item.get('id')

        if item_type == 'word':
            word_count += 1

            # æ£€æŸ¥å•è¯
            issues = check_spelling_basic(content)
            if not definition:
                issues.append("ç¼ºå°‘å®šä¹‰")

            if issues:
                word_issues.append({
                    'id': item_id,
                    'content': content,
                    'phonetic': phonetic,
                    'definition': definition,
                    'issues': issues
                })

        elif item_type == 'sentence':
            sentence_count += 1

            # æ£€æŸ¥å¥å­
            issues = check_sentence(content)
            trans_issues = check_translation(translation)
            issues.extend(trans_issues)

            if issues:
                sentence_issues.append({
                    'id': item_id,
                    'content': content,
                    'translation': translation,
                    'issues': issues
                })

    # è¾“å‡ºç»Ÿè®¡
    print("=" * 80)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 80)
    print(f"å•è¯æ€»æ•°: {word_count}")
    print(f"å¥å­æ€»æ•°: {sentence_count}")
    print(f"æœ‰é—®é¢˜çš„å•è¯: {len(word_issues)}")
    print(f"æœ‰é—®é¢˜çš„å¥å­: {len(sentence_issues)}")
    print()

    # è¾“å‡ºæœ‰é—®é¢˜çš„å•è¯
    if word_issues:
        print("=" * 80)
        print("âš ï¸  æœ‰é—®é¢˜çš„å•è¯")
        print("=" * 80)
        for item in word_issues:
            print(f"\nID: {item['id']}")
            print(f"å•è¯: {item['content']}")
            if item['phonetic']:
                print(f"éŸ³æ ‡: {item['phonetic']}")
            if item['definition']:
                print(f"å®šä¹‰: {item['definition']}")
            print("é—®é¢˜:")
            for issue in item['issues']:
                print(f"  - {issue}")
            print("-" * 80)
        print()

    # è¾“å‡ºæœ‰é—®é¢˜çš„å¥å­
    if sentence_issues:
        print("=" * 80)
        print("âš ï¸  æœ‰é—®é¢˜çš„å¥å­")
        print("=" * 80)
        for item in sentence_issues:
            print(f"\nID: {item['id']}")
            print(f"å¥å­: {item['content']}")
            if item['translation']:
                print(f"ç¿»è¯‘: {item['translation']}")
            print("é—®é¢˜:")
            for issue in item['issues']:
                print(f"  - {issue}")
            print("-" * 80)
        print()

    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    output_file = f"/home/lichangjiang/.openclaw/workspace/srs_check_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("SRS Word æ‹¼å†™å’Œç¿»è¯‘æ£€æŸ¥æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n")
        f.write(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"å•è¯æ€»æ•°: {word_count}\n")
        f.write(f"å¥å­æ€»æ•°: {sentence_count}\n")
        f.write(f"æœ‰é—®é¢˜çš„å•è¯: {len(word_issues)}\n")
        f.write(f"æœ‰é—®é¢˜çš„å¥å­: {len(sentence_issues)}\n\n")

        if word_issues:
            f.write("=" * 80 + "\n")
            f.write("æœ‰é—®é¢˜çš„å•è¯\n")
            f.write("=" * 80 + "\n\n")
            for item in word_issues:
                f.write(f"\nID: {item['id']}\n")
                f.write(f"å•è¯: {item['content']}\n")
                if item['phonetic']:
                    f.write(f"éŸ³æ ‡: {item['phonetic']}\n")
                if item['definition']:
                    f.write(f"å®šä¹‰: {item['definition']}\n")
                f.write("é—®é¢˜:\n")
                for issue in item['issues']:
                    f.write(f"  - {issue}\n")
                f.write("-" * 80 + "\n")

        if sentence_issues:
            f.write("\n" + "=" * 80 + "\n")
            f.write("æœ‰é—®é¢˜çš„å¥å­\n")
            f.write("=" * 80 + "\n\n")
            for item in sentence_issues:
                f.write(f"\nID: {item['id']}\n")
                f.write(f"å¥å­: {item['content']}\n")
                if item['translation']:
                    f.write(f"ç¿»è¯‘: {item['translation']}\n")
                f.write("é—®é¢˜:\n")
                for issue in item['issues']:
                    f.write(f"  - {issue}\n")
                f.write("-" * 80 + "\n")

    print(f"âœ… æ£€æŸ¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

    # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè¾“å‡ºæˆåŠŸä¿¡æ¯
    if not word_issues and not sentence_issues:
        print()
        print("=" * 80)
        print("ğŸ‰ æ£€æŸ¥å®Œæˆï¼æ²¡æœ‰å‘ç°æ˜æ˜¾çš„æ‹¼å†™æˆ–ç¿»è¯‘é—®é¢˜ï¼")
        print("=" * 80)

if __name__ == '__main__':
    main()
